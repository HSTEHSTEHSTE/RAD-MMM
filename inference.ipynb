{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5c0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to RAD-MMM inference tutorial\n",
    "\n",
    "## imports\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "import yaml\n",
    "sys.path.append('/akshit/scratch/RAD-MMM/vocoders')\n",
    "sys.path.append('/akshit/scratch/RAD-MMM')\n",
    "from pytorch_lightning.cli import LightningCLI\n",
    "from tts_lightning_modules import TTSModel\n",
    "from data_modules import BaseAudioDataModule\n",
    "from jsonargparse import lazy_instance\n",
    "from decoders import RADMMMFlow\n",
    "from loss import RADTTSLoss\n",
    "import inspect\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from training_callbacks import LogDecoderSamplesCallback, \\\n",
    "    LogAttributeSamplesCallback\n",
    "from utils import get_class_args\n",
    "from tts_text_processing.text_processing import TextProcessing\n",
    "from common import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2738468b-2171-4137-9c28-4702b5db9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tts_main import RADTTSLightningCLI,lcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05e7b4-1561-4cf0-9953-50ea676ab33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6c72a3-2e28-47c6-bb9d-4e0b53845456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "attribute_model_path = \"/akshit/scratch/generator_ckpt/radmmm_public/attribute_model.ckpt\"\n",
    "decoder_model_path = \"/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\"\n",
    "gen_config_path = \"/akshit/scratch/generator_ckpt/radmmm_public/config_interactive.yaml\"\n",
    "voc_model_path = \"/akshit/scratch/generator_ckpt/hfg_public/g_00072000\"\n",
    "voc_config_path = \"/akshit/scratch/generator_ckpt/hfg_public/config_16khz.json\"\n",
    "phonemizer_cfg='{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5356651-8a00-4440-acb9-5257428620fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f90560-816d-4c65-b59a-915b28fc1a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import mock\n",
    "\n",
    "# with mock.patch(\"sys.argv\", [\"predict\", \"--config=\"+gen_config_path,\\\n",
    "#                              # \"--ckpt_path=\"+attribute_model_path,\\\n",
    "#                              \"--model.predict_mode=tts\", \\\n",
    "#                              \"--data.inference_transcript=model_inputs/resynthesis_prompts.json\", \\\n",
    "#                              \"--model.prediction_output_dir=/akshit/scratch/RAD-MMM/tutorials/out1\", \\\n",
    "#                              \"--trainer.devices=1\", \"--data.batch_size=1\", \\\n",
    "#                              \"--model.vocoder_checkpoint_path=\"+voc_model_path, \\\n",
    "#                              \"--model.vocoder_config_path=\"+voc_config_path, \\\n",
    "#                              \"--data.phonemizer_cfg=\"+'{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}', \\\n",
    "#                              \"--model.encoders_path=\"+decoder_model_path, \\\n",
    "#                              \"--model.decoder_path=\"+decoder_model_path, \\\n",
    "#                              \"--model.output_directory=/akshit/scratch/RAD-MMM/tutorials/run1\"]):\n",
    "#     cli = RADTTSLightningCLI(TTSModel, BaseAudioDataModule, save_config_kwargs={\"overwrite\": True}, run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bf7184-cc86-4fa1-9d6f-7306ccc624e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results in issue - SIGSEV\n",
    "\n",
    "# cli.trainer.predict(model=cli.model,datamodule=cli.datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50cd070-2311-42d8-82ed-1352e3c96188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, state_dict: Mapping[str, Any], strict: bool = True)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempting to load the model\n",
    "import pytorch_lightning\n",
    "import inspect\n",
    "import torch\n",
    "from decoders import RADMMMFlow\n",
    "# inspect.signature(pytorch_lightning.cli.instantiate_module)\n",
    "inspect.signature(torch.nn.Module.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edddca9-4d40-456e-9a80-cc438859255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/akshit/scratch/RAD-MMM/tutorials/run1/lightning_logs/version_12/hparams.yaml\", \"r\") as f:\n",
    "    hparams = yaml.safe_load(f)\n",
    "\n",
    "with open(gen_config_path, \"r\") as f:\n",
    "    gen_config = yaml.safe_load(f)\n",
    "# hparams['decoder']['init_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b86dbb4-be11-412b-979b-ad13ec9e52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mauvilsa/lightning/blob/c71406f390b89be7bbe3415e57dd785095675779/src/lightning/pytorch/cli.py\n",
    "def instantiate_class(init):\n",
    "    \"\"\"Instantiates a class with the given args and init.\n",
    "\n",
    "    Args:\n",
    "        args: Positional arguments required for instantiation.\n",
    "        init: Dict of the form {\"class_path\":...,\"init_args\":...}.\n",
    "\n",
    "    Returns:\n",
    "        The instantiated class object.\n",
    "    \"\"\"\n",
    "    kwargs = init.get(\"init_args\", {})\n",
    "    class_module, class_name = init[\"class_path\"].rsplit(\".\", 1)\n",
    "    module = __import__(class_module, fromlist=[class_name])\n",
    "    args_class = getattr(module, class_name)\n",
    "    return args_class(**kwargs)\n",
    "    \n",
    "# https://lightning.ai/forums/t/best-way-to-use-load-from-checkpoint-when-model-contains-other-models/2094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15c16de-ca51-423f-a718-0a17ea93884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/akshit/scratch/RAD-MMM/common.py:557: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:2425.)\n",
      "  W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:1728: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "LU, pivots = torch.lu(A, compute_pivots)\n",
      "should be replaced with\n",
      "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "and\n",
      "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "should be replaced with\n",
      "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/BatchLinearAlgebra.cpp:1991.)\n",
      "  return torch._lu_with_info(A, pivot=pivot, check_errors=(not get_infos))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_loss\n",
      "text_encoder\n",
      "Applying spectral norm to text encoder LSTM\n",
      "f0_predictor\n",
      "f0_predictor_loss\n",
      "energy_predictor\n",
      "energy_predictor_loss\n",
      "voiced_predictor\n",
      "voiced_predictor_loss\n",
      "duration_predictor\n",
      "duration_predictor_loss\n",
      "speaker_embed_regularization_loss\n",
      "speaker_accent_cross_regularization_loss\n"
     ]
    }
   ],
   "source": [
    "ttsmodel_kwargs={}\n",
    "for k,v in hparams.items():\n",
    "    if type(v) == dict and 'class_path' in v:\n",
    "        print(k)\n",
    "        ttsmodel_kwargs[k] = instantiate_class(v)\n",
    "    elif k != \"_instantiator\":\n",
    "        ttsmodel_kwargs[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537eefd4-b1a5-42e5-b406-24360e90896e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder': RADMMMFlow(\n",
       "   (length_regulator): LengthRegulator()\n",
       "   (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
       "   (flows): ModuleList(\n",
       "     (0): FlowStep(\n",
       "       (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (1): FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (2-3): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (4-5): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (6-7): 2 x FlowStep(\n",
       "       (invtbl_conv): Invertible1x1ConvLUS()\n",
       "       (coupling_tfn): AffineTransformationLayer(\n",
       "         (affine_param_predictor): WN(\n",
       "           (in_layers): ModuleList(\n",
       "             (0): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "             )\n",
       "             (1): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "             )\n",
       "             (2): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "             )\n",
       "             (3): ConvNorm(\n",
       "               (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "             )\n",
       "           )\n",
       "           (res_skip_layers): ModuleList(\n",
       "             (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "           )\n",
       "           (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
       "           (softplus): Softplus(beta=1, threshold=20)\n",
       "           (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
       " ),\n",
       " 'decoder_loss': RADMMMLoss(\n",
       "   (attn_loss): AttentionLoss(\n",
       "     (attn_ctc_loss): AttentionCTCLoss(\n",
       "       (log_softmax): LogSoftmax(dim=3)\n",
       "       (CTCLoss): CTCLoss()\n",
       "     )\n",
       "     (attn_bin_loss): AttentionBinarizationLoss()\n",
       "   )\n",
       " ),\n",
       " 'text_encoder': Encoder(\n",
       "   (convolutions): ModuleList(\n",
       "     (0-2): 3 x Sequential(\n",
       "       (0): ConvNorm(\n",
       "         (conv): PartialConv1d(520, 520, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1): InstanceNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "     )\n",
       "   )\n",
       "   (lstm): LSTM(520, 260, batch_first=True, bidirectional=True)\n",
       " ),\n",
       " 'f0_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'f0_predictor_loss': AttributeRegressionLoss(),\n",
       " 'energy_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'energy_predictor_loss': AttributeRegressionLoss(),\n",
       " 'voiced_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'voiced_predictor_loss': AttributeRegressionLoss(),\n",
       " 'duration_predictor': ConvLSTMLinearDAP(\n",
       "   (bottleneck_layer): BottleneckLayer(\n",
       "     (projection_fn): ConvNorm(\n",
       "       (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "     )\n",
       "     (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "   )\n",
       "   (feat_pred_fn): ConvLSTMLinear(\n",
       "     (dropout): Dropout(p=0.5, inplace=False)\n",
       "     (convolutions): ModuleList(\n",
       "       (0): ConvNorm(\n",
       "         (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "       )\n",
       "       (1-2): 2 x ConvNorm(\n",
       "         (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "       )\n",
       "     )\n",
       "     (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "     (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'duration_predictor_loss': AttributeRegressionLoss(),\n",
       " 'speaker_embed_regularization_loss': VarianceCovarianceEmbeddingRegLoss(),\n",
       " 'speaker_accent_cross_regularization_loss': AttributeMinCrossCovarianceRegLoss(),\n",
       " 'optim_algo': 'RAdam',\n",
       " 'learning_rate': 0.0001,\n",
       " 'weight_decay': 1e-06,\n",
       " 'sigma': 1.0,\n",
       " 'iters_per_checkpoint': 3000,\n",
       " 'unfreeze_modules': 'all',\n",
       " 'binarization_start_iter': 0,\n",
       " 'output_directory': '/akshit/scratch/RAD-MMM/tutorials/run1',\n",
       " 'log_decoder_samples': True,\n",
       " 'scale_mel': True,\n",
       " 'vocoder_config_path': '/akshit/scratch/generator_ckpt/hfg_public/config_16khz.json',\n",
       " 'vocoder_checkpoint_path': '/akshit/scratch/generator_ckpt/hfg_public/g_00072000',\n",
       " 'p_estimate_ambiguous_phonemes': 0.0,\n",
       " 'phoneme_estimation_start_iter': 1000,\n",
       " 'decoder_path': '/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt',\n",
       " 'encoders_path': '/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt',\n",
       " 'f0_loss_voiced_only': True,\n",
       " 'n_speakers': 7,\n",
       " 'n_speaker_dim': 16,\n",
       " 'use_accent': True,\n",
       " 'n_accents': 7,\n",
       " 'n_accent_dim': 8,\n",
       " 'n_text_dim': 512,\n",
       " 'n_text_tokens': 426,\n",
       " 'lstm_norm_fn': 'spectral',\n",
       " 'n_mel_channels': 80,\n",
       " 'use_syncbnorm': False,\n",
       " 'prediction_output_dir': '/akshit/scratch/RAD-MMM/tutorials/out1',\n",
       " 'predict_mode': 'tts',\n",
       " 'use_accent_emb_for_encoder': True,\n",
       " 'use_accent_emb_for_decoder': False,\n",
       " 'use_accent_emb_for_alignment': False,\n",
       " 'use_speaker_emb_for_alignment': True,\n",
       " 'n_augmentations': 2,\n",
       " 'sampling_rate': 16000,\n",
       " 'symbol_set': 'radmmm_phonemizer_marker_segregated',\n",
       " 'cleaner_names': ['radtts_cleaners'],\n",
       " 'heteronyms_path': 'tts_text_processing/heteronyms',\n",
       " 'phoneme_dict_path': 'tts_text_processing/cmudict-0.7b',\n",
       " 'p_phoneme': 1.0,\n",
       " 'handle_phoneme': 'word',\n",
       " 'handle_phoneme_ambiguous': 'ignore',\n",
       " 'prepend_space_to_text': True,\n",
       " 'append_space_to_text': True,\n",
       " 'add_bos_eos_to_text': False,\n",
       " 'phonemizer_cfg': '{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttsmodel_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcbb65e-ba65-44c8-bfb9-c87e5137c393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model2=TTSModel(**ttsmodel_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68099cea-9541-44d4-be96-5d55a2954afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_config[\"data\"][\"inference_transcript\"]=\"model_inputs/resynthesis_prompts.json\" #ToDo\n",
    "gen_config[\"data\"][\"batch_size\"]=1\n",
    "gen_config[\"data\"][\"phonemizer_cfg\"]=phonemizer_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e6465f2-868f-4da7-ab26-e9b871a9d41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decoder',\n",
       " 'text_embeddings',\n",
       " 'text_encoder',\n",
       " 'speaker_embeddings',\n",
       " 'attention',\n",
       " 'accent_embeddings']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.pretrained_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40fbd26a-f034-495a-b5ba-b4b01d769afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tload_attr = torch.load(attribute_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc944aa-f903-4ae3-ac4d-cf46a56613d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['f0_predictor.bottleneck_layer.projection_fn.conv.bias', 'f0_predictor.bottleneck_layer.projection_fn.conv.weight_g', 'f0_predictor.bottleneck_layer.projection_fn.conv.weight_v', 'f0_predictor.feat_pred_fn.convolutions.0.conv.bias', 'f0_predictor.feat_pred_fn.convolutions.0.conv.weight_g', 'f0_predictor.feat_pred_fn.convolutions.0.conv.weight_v', 'f0_predictor.feat_pred_fn.convolutions.1.conv.bias', 'f0_predictor.feat_pred_fn.convolutions.1.conv.weight_g', 'f0_predictor.feat_pred_fn.convolutions.1.conv.weight_v', 'f0_predictor.feat_pred_fn.convolutions.2.conv.bias', 'f0_predictor.feat_pred_fn.convolutions.2.conv.weight_g', 'f0_predictor.feat_pred_fn.convolutions.2.conv.weight_v', 'f0_predictor.feat_pred_fn.bilstm.weight_ih_l0', 'f0_predictor.feat_pred_fn.bilstm.bias_ih_l0', 'f0_predictor.feat_pred_fn.bilstm.bias_hh_l0', 'f0_predictor.feat_pred_fn.bilstm.weight_ih_l0_reverse', 'f0_predictor.feat_pred_fn.bilstm.bias_ih_l0_reverse', 'f0_predictor.feat_pred_fn.bilstm.bias_hh_l0_reverse', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_orig', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_orig', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_u', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_v', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_u', 'f0_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_v', 'f0_predictor.feat_pred_fn.dense.weight', 'f0_predictor.feat_pred_fn.dense.bias', 'energy_predictor.bottleneck_layer.projection_fn.conv.bias', 'energy_predictor.bottleneck_layer.projection_fn.conv.weight_g', 'energy_predictor.bottleneck_layer.projection_fn.conv.weight_v', 'energy_predictor.feat_pred_fn.convolutions.0.conv.bias', 'energy_predictor.feat_pred_fn.convolutions.0.conv.weight_g', 'energy_predictor.feat_pred_fn.convolutions.0.conv.weight_v', 'energy_predictor.feat_pred_fn.convolutions.1.conv.bias', 'energy_predictor.feat_pred_fn.convolutions.1.conv.weight_g', 'energy_predictor.feat_pred_fn.convolutions.1.conv.weight_v', 'energy_predictor.feat_pred_fn.convolutions.2.conv.bias', 'energy_predictor.feat_pred_fn.convolutions.2.conv.weight_g', 'energy_predictor.feat_pred_fn.convolutions.2.conv.weight_v', 'energy_predictor.feat_pred_fn.bilstm.weight_ih_l0', 'energy_predictor.feat_pred_fn.bilstm.bias_ih_l0', 'energy_predictor.feat_pred_fn.bilstm.bias_hh_l0', 'energy_predictor.feat_pred_fn.bilstm.weight_ih_l0_reverse', 'energy_predictor.feat_pred_fn.bilstm.bias_ih_l0_reverse', 'energy_predictor.feat_pred_fn.bilstm.bias_hh_l0_reverse', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_orig', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_orig', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_u', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_v', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_u', 'energy_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_v', 'energy_predictor.feat_pred_fn.dense.weight', 'energy_predictor.feat_pred_fn.dense.bias', 'voiced_predictor.bottleneck_layer.projection_fn.conv.bias', 'voiced_predictor.bottleneck_layer.projection_fn.conv.weight_g', 'voiced_predictor.bottleneck_layer.projection_fn.conv.weight_v', 'voiced_predictor.feat_pred_fn.convolutions.0.conv.bias', 'voiced_predictor.feat_pred_fn.convolutions.0.conv.weight_g', 'voiced_predictor.feat_pred_fn.convolutions.0.conv.weight_v', 'voiced_predictor.feat_pred_fn.convolutions.1.conv.bias', 'voiced_predictor.feat_pred_fn.convolutions.1.conv.weight_g', 'voiced_predictor.feat_pred_fn.convolutions.1.conv.weight_v', 'voiced_predictor.feat_pred_fn.convolutions.2.conv.bias', 'voiced_predictor.feat_pred_fn.convolutions.2.conv.weight_g', 'voiced_predictor.feat_pred_fn.convolutions.2.conv.weight_v', 'voiced_predictor.feat_pred_fn.bilstm.weight_ih_l0', 'voiced_predictor.feat_pred_fn.bilstm.bias_ih_l0', 'voiced_predictor.feat_pred_fn.bilstm.bias_hh_l0', 'voiced_predictor.feat_pred_fn.bilstm.weight_ih_l0_reverse', 'voiced_predictor.feat_pred_fn.bilstm.bias_ih_l0_reverse', 'voiced_predictor.feat_pred_fn.bilstm.bias_hh_l0_reverse', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_orig', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_orig', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_u', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_v', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_u', 'voiced_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_v', 'voiced_predictor.feat_pred_fn.dense.weight', 'voiced_predictor.feat_pred_fn.dense.bias', 'duration_predictor.bottleneck_layer.projection_fn.conv.bias', 'duration_predictor.bottleneck_layer.projection_fn.conv.weight_g', 'duration_predictor.bottleneck_layer.projection_fn.conv.weight_v', 'duration_predictor.feat_pred_fn.convolutions.0.conv.bias', 'duration_predictor.feat_pred_fn.convolutions.0.conv.weight_g', 'duration_predictor.feat_pred_fn.convolutions.0.conv.weight_v', 'duration_predictor.feat_pred_fn.convolutions.1.conv.bias', 'duration_predictor.feat_pred_fn.convolutions.1.conv.weight_g', 'duration_predictor.feat_pred_fn.convolutions.1.conv.weight_v', 'duration_predictor.feat_pred_fn.convolutions.2.conv.bias', 'duration_predictor.feat_pred_fn.convolutions.2.conv.weight_g', 'duration_predictor.feat_pred_fn.convolutions.2.conv.weight_v', 'duration_predictor.feat_pred_fn.bilstm.weight_ih_l0', 'duration_predictor.feat_pred_fn.bilstm.bias_ih_l0', 'duration_predictor.feat_pred_fn.bilstm.bias_hh_l0', 'duration_predictor.feat_pred_fn.bilstm.weight_ih_l0_reverse', 'duration_predictor.feat_pred_fn.bilstm.bias_ih_l0_reverse', 'duration_predictor.feat_pred_fn.bilstm.bias_hh_l0_reverse', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_orig', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_orig', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_u', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_v', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_u', 'duration_predictor.feat_pred_fn.bilstm.weight_hh_l0_reverse_v', 'duration_predictor.feat_pred_fn.dense.weight', 'duration_predictor.feat_pred_fn.dense.bias'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tload_attr['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45290a35-c414-4de5-a886-ffab4f84b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for modules in model2.pretrained_modules:\n",
    "    to_delete += [k for k in tload_attr['state_dict'].keys() if k[:len(modules)] == modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b17d507-95d6-4718-bd27-18c550e40f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fced81f-6f49-4593-ac77-753c31afed9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c366d31-416e-4346-8589-0db6c8cfa2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  assets/en_US_word_ipa_map.txt\n",
      "loading:  assets/es_MX_word_ipa_map.txt\n",
      "loading:  assets/de_DE_word_ipa_map.txt\n",
      "loading:  assets/en_UK_word_ipa_map.txt\n",
      "loading:  assets/es_CO_word_ipa_map.txt\n",
      "loading:  assets/es_ES_word_ipa_map.txt\n",
      "loading:  assets/fr_FR_word_ipa_map.txt\n",
      "loading:  assets/hi_HI_word_ipa_map.txt\n",
      "loading:  assets/pt_BR_word_ipa_map.txt\n",
      "loading:  assets/te_TE_word_ipa_map.txt\n",
      "Number of symbols: 439\n",
      "updating the speakers set: 7\n",
      "Initializing f0 predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing energy predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing voiced_predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing duration_predictor\n",
      "ConvLSTMLinearDAP(\n",
      "  (bottleneck_layer): BottleneckLayer(\n",
      "    (projection_fn): ConvNorm(\n",
      "      (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (non_linearity): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (feat_pred_fn): ConvLSTMLinear(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (convolutions): ModuleList(\n",
      "      (0): ConvNorm(\n",
      "        (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "      (1-2): 2 x ConvNorm(\n",
      "        (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "      )\n",
      "    )\n",
      "    (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initializing Speaker Regularization Component\n",
      "Initializing Speaker<>Accent Regularization Component\n",
      "HIFIGAN loading...\n",
      "RADMMMFlow(\n",
      "  (length_regulator): LengthRegulator()\n",
      "  (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
      "  (flows): ModuleList(\n",
      "    (0): FlowStep(\n",
      "      (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2-3): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4-5): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6-7): 2 x FlowStep(\n",
      "      (invtbl_conv): Invertible1x1ConvLUS()\n",
      "      (coupling_tfn): AffineTransformationLayer(\n",
      "        (affine_param_predictor): WN(\n",
      "          (in_layers): ModuleList(\n",
      "            (0): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            )\n",
      "            (1): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "            )\n",
      "            (2): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "            )\n",
      "            (3): ConvNorm(\n",
      "              (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "            )\n",
      "          )\n",
      "          (res_skip_layers): ModuleList(\n",
      "            (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
      "          (softplus): Softplus(beta=1, threshold=20)\n",
      "          (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
      ")\n",
      "/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\n",
      "Loaded pretrained decoder\n",
      "/akshit/scratch/generator_ckpt/radmmm_public/decoder.ckpt\n",
      "Loaded pretrained text, speaker, attention modules\n",
      "Module attention not loaded from checkpoint\n",
      "Module speaker_embeddings not loaded from checkpoint\n",
      "Module text_embeddings not loaded from checkpoint\n",
      "Module accent_embeddings not loaded from checkpoint\n",
      "Module text_encoder not loaded from checkpoint\n",
      "Module decoder not loaded from checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = TTSModel.load_from_checkpoint(checkpoint_path=attribute_model_path,\\\n",
    "                                      **ttsmodel_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81bd7b8-01da-4c68-b214-2781a375033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  assets/en_US_word_ipa_map.txt\n",
      "loading:  assets/es_MX_word_ipa_map.txt\n",
      "loading:  assets/de_DE_word_ipa_map.txt\n",
      "loading:  assets/en_UK_word_ipa_map.txt\n",
      "loading:  assets/es_CO_word_ipa_map.txt\n",
      "loading:  assets/es_ES_word_ipa_map.txt\n",
      "loading:  assets/fr_FR_word_ipa_map.txt\n",
      "loading:  assets/hi_HI_word_ipa_map.txt\n",
      "loading:  assets/pt_BR_word_ipa_map.txt\n",
      "loading:  assets/te_TE_word_ipa_map.txt\n",
      "Number of symbols: 439\n"
     ]
    }
   ],
   "source": [
    "data_module = BaseAudioDataModule(**gen_config['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d766a7-c22c-443d-954f-4156597cccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/LJSpeech-1.0', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'LJSpeech/ljs_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt', 'language': 'en_US', 'phonemized': True}\n",
      "processing file: datasets/opensource/LJSpeech/ljs_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/HUI-Audio-Corpus-German/Bernd_Ungerer', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'HUI-Audio-Corpus-German/Bernd_Ungerer/berndungerer_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'de_DE', 'phonemized': True}\n",
      "processing file: datasets/opensource/HUI-Audio-Corpus-German/Bernd_Ungerer/berndungerer_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/es_ES/male/tux', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/es_ES/male/tux/tux_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'es_ES', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/es_ES/male/tux/tux_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/es_MX/female/karen_savage', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/es_MX/female/karen_savage/ks_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt', 'language': 'es_MX', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/es_MX/female/karen_savage/ks_audiopath_text_sid_emotion_duration_train_filelist_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/MAILABS/fr_FR/female/nadine_eckert_boulet/', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'MAILABS/fr_FR/female/nadine_eckert_boulet/nadineeckert_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt', 'language': 'fr_FR', 'phonemized': True}\n",
      "processing file: datasets/opensource/MAILABS/fr_FR/female/nadine_eckert_boulet/nadineeckert_audiopath_text_sid_emotion_duration_train_filelist_filtered_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/indic-languages-tts-iiit-h', 'sampling_rate': '16khz_single_channel', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'indic-languages-tts-iiit-h/hi_indic_iiit_hyderbad_audiopath_text_sid_emotion_duration_train_phonemized.txt', 'language': 'hi_HI', 'phonemized': True}\n",
      "processing file: datasets/opensource/indic-languages-tts-iiit-h/hi_indic_iiit_hyderbad_audiopath_text_sid_emotion_duration_train_phonemized.txt\n",
      "{'basedir': '/home/dcg-adlr-rbadlani-data/multilingual-dataset/opensource/TTS-Portuguese-Corpus/', 'sampling_rate': '16khz', 'filelist_basedir': 'datasets/opensource/', 'filelist': 'TTS-Portuguese-Corpus/ed_portuguese_audiopath_transcript_sid_emotion_duration_train_phonemized.txt', 'language': 'pt_BR', 'phonemized': True}\n",
      "processing file: datasets/opensource/TTS-Portuguese-Corpus/ed_portuguese_audiopath_transcript_sid_emotion_duration_train_phonemized.txt\n",
      "Number of speakers : 7\n",
      "speaker ids: {'ED-other': 0, 'hui-berndungerer-other': 1, 'indic-iiit-hyd-female-other': 2, 'ljs-other': 3, 'mailabs-karensavage-other': 4, 'mailabs-nadineeckert-other': 5, 'mailabs-tux-other': 6}\n",
      "Number of languages : 7\n",
      "language ids: {'de_DE': 0, 'en_US': 1, 'es_ES': 2, 'es_MX': 3, 'fr_FR': 4, 'hi_HI': 5, 'pt_BR': 6}\n",
      "Number of files 65197\n",
      "Include emotion ['other']: True\n",
      "Number of files after emotion filtering 65197\n",
      "Number of files after duration filtering 54096\n",
      "Dataloader initialized with no augmentations\n",
      "{'mailabs-tux-other': {'f0_median': 122.66626739501953, 'f0_mean': 124.51791381835938, 'f0_std': 24.38829803466797, 'log_f0_median': 4.809467315673828, 'log_f0_mean': 4.811684608459473, 'log_f0_std': 0.15116865932941437, 'energy_mean': 0.9740287661552429, 'energy_std': 0.06282375752925873, 'n_files': 100}, 'ed-other': {'f0_median': 150.15008544921875, 'f0_mean': 152.173828125, 'f0_std': 30.257091522216797, 'log_f0_median': 5.0116353034973145, 'log_f0_mean': 5.004810810089111, 'log_f0_std': 0.2032385766506195, 'energy_mean': 0.9661457538604736, 'energy_std': 0.03739410266280174, 'n_files': 100}, 'hui-berndungerer-other': {'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}, 'indic-iiit-hyd-female-other': {'f0_median': 265.9961242675781, 'f0_mean': 279.2513122558594, 'f0_std': 57.07512283325195, 'log_f0_median': 5.583481788635254, 'log_f0_mean': 5.612209320068359, 'log_f0_std': 0.1978427916765213, 'energy_mean': 0.9863940477371216, 'energy_std': 0.029389921575784683, 'n_files': 100}, 'ljs-other': {'f0_median': 203.92970275878906, 'f0_mean': 211.856201171875, 'f0_std': 51.960838317871094, 'log_f0_median': 5.317775249481201, 'log_f0_mean': 5.328337669372559, 'log_f0_std': 0.231437548995018, 'energy_mean': 0.9916158318519592, 'energy_std': 0.029046861454844475, 'n_files': 100}, 'mailabs-karensavage-other': {'f0_median': 207.49432373046875, 'f0_mean': 210.6017608642578, 'f0_std': 28.416366577148438, 'log_f0_median': 5.335103988647461, 'log_f0_mean': 5.341104507446289, 'log_f0_std': 0.13288341462612152, 'energy_mean': 1.0020556449890137, 'energy_std': 0.0282765943557024, 'n_files': 100}, 'mailabs-nadineeckert-other': {'f0_median': 191.37538146972656, 'f0_mean': 200.55995178222656, 'f0_std': 50.84891891479492, 'log_f0_median': 5.254236698150635, 'log_f0_mean': 5.272150993347168, 'log_f0_std': 0.23656471073627472, 'energy_mean': 0.9833611249923706, 'energy_std': 0.03346679359674454, 'n_files': 100}}\n",
      "{'mailabs-tux-other': {'f0_median': 122.66626739501953, 'f0_mean': 124.51791381835938, 'f0_std': 24.38829803466797, 'log_f0_median': 4.809467315673828, 'log_f0_mean': 4.811684608459473, 'log_f0_std': 0.15116865932941437, 'energy_mean': 0.9740287661552429, 'energy_std': 0.06282375752925873, 'n_files': 100}, 'ed-other': {'f0_median': 150.15008544921875, 'f0_mean': 152.173828125, 'f0_std': 30.257091522216797, 'log_f0_median': 5.0116353034973145, 'log_f0_mean': 5.004810810089111, 'log_f0_std': 0.2032385766506195, 'energy_mean': 0.9661457538604736, 'energy_std': 0.03739410266280174, 'n_files': 100}, 'hui-berndungerer-other': {'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}, 'indic-iiit-hyd-female-other': {'f0_median': 265.9961242675781, 'f0_mean': 279.2513122558594, 'f0_std': 57.07512283325195, 'log_f0_median': 5.583481788635254, 'log_f0_mean': 5.612209320068359, 'log_f0_std': 0.1978427916765213, 'energy_mean': 0.9863940477371216, 'energy_std': 0.029389921575784683, 'n_files': 100}, 'ljs-other': {'f0_median': 203.92970275878906, 'f0_mean': 211.856201171875, 'f0_std': 51.960838317871094, 'log_f0_median': 5.317775249481201, 'log_f0_mean': 5.328337669372559, 'log_f0_std': 0.231437548995018, 'energy_mean': 0.9916158318519592, 'energy_std': 0.029046861454844475, 'n_files': 100}, 'mailabs-karensavage-other': {'f0_median': 207.49432373046875, 'f0_mean': 210.6017608642578, 'f0_std': 28.416366577148438, 'log_f0_median': 5.335103988647461, 'log_f0_mean': 5.341104507446289, 'log_f0_std': 0.13288341462612152, 'energy_mean': 1.0020556449890137, 'energy_std': 0.0282765943557024, 'n_files': 100}, 'mailabs-nadineeckert-other': {'f0_median': 191.37538146972656, 'f0_mean': 200.55995178222656, 'f0_std': 50.84891891479492, 'log_f0_median': 5.254236698150635, 'log_f0_mean': 5.272150993347168, 'log_f0_std': 0.23656471073627472, 'energy_mean': 0.9833611249923706, 'energy_std': 0.03346679359674454, 'n_files': 100}}\n"
     ]
    }
   ],
   "source": [
    "data_module.setup(\"predict\")\n",
    "# dm.teardown(stage=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91defc0-a41c-4412-afeb-d823b0cffa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.predictset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486ee7d-b1e5-42f6-ae10-fc83506372bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.tp_inference.phonemizer_backend_dict\n",
    "data_module.tp.phonemizer_backend_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd992cdd-c308-445a-9fc4-f5fe89c04498",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dl = data_module.predict_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05440126-73e6-4a2f-9f78-17845465b765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTSModel(\n",
       "  (text_embeddings): Embedding(426, 512)\n",
       "  (text_encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): PartialConv1d(520, 520, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): InstanceNorm1d(520, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(520, 260, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (speaker_embeddings): Embedding(21, 16)\n",
       "  (accent_embeddings): Embedding(7, 8)\n",
       "  (attention): ConvAttention(\n",
       "    (softmax): Softmax(dim=3)\n",
       "    (log_softmax): LogSoftmax(dim=3)\n",
       "    (key_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(528, 1056, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(1056, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (query_proj): Sequential(\n",
       "      (0): ConvNorm(\n",
       "        (conv): Conv1d(80, 160, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): ConvNorm(\n",
       "        (conv): Conv1d(160, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (3): ReLU()\n",
       "      (4): ConvNorm(\n",
       "        (conv): Conv1d(80, 80, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RADMMMFlow(\n",
       "    (length_regulator): LengthRegulator()\n",
       "    (context_lstm): LSTM(1060, 528, batch_first=True, bidirectional=True)\n",
       "    (flows): ModuleList(\n",
       "      (0): FlowStep(\n",
       "        (invtbl_conv): DataInitializedInvertible1x1Conv()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1136, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1135, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 158, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1134, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 156, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6-7): 2 x FlowStep(\n",
       "        (invtbl_conv): Invertible1x1ConvLUS()\n",
       "        (coupling_tfn): AffineTransformationLayer(\n",
       "          (affine_param_predictor): WN(\n",
       "            (in_layers): ModuleList(\n",
       "              (0): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "              )\n",
       "              (1): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "              )\n",
       "              (2): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "              )\n",
       "              (3): ConvNorm(\n",
       "                (conv): PartialConv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
       "              )\n",
       "            )\n",
       "            (res_skip_layers): ModuleList(\n",
       "              (0-3): 4 x Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (start): Conv1d(1133, 1024, kernel_size=(1,), stride=(1,))\n",
       "            (softplus): Softplus(beta=1, threshold=20)\n",
       "            (end): Conv1d(1024, 154, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (unfold): Unfold(kernel_size=(2, 1), dilation=1, padding=0, stride=2)\n",
       "  )\n",
       "  (decoder_criterion): RADMMMLoss(\n",
       "    (attn_loss): AttentionLoss(\n",
       "      (attn_ctc_loss): AttentionCTCLoss(\n",
       "        (log_softmax): LogSoftmax(dim=3)\n",
       "        (CTCLoss): CTCLoss()\n",
       "      )\n",
       "      (attn_bin_loss): AttentionBinarizationLoss()\n",
       "    )\n",
       "  )\n",
       "  (f0_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (f0_predictor_loss): AttributeRegressionLoss()\n",
       "  (energy_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (energy_predictor_loss): AttributeRegressionLoss()\n",
       "  (voiced_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (voiced_predictor_loss): AttributeRegressionLoss()\n",
       "  (duration_predictor): ConvLSTMLinearDAP(\n",
       "    (bottleneck_layer): BottleneckLayer(\n",
       "      (projection_fn): ConvNorm(\n",
       "        (conv): Conv1d(520, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "      (non_linearity): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (feat_pred_fn): ConvLSTMLinear(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (convolutions): ModuleList(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(48, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "        (1-2): 2 x ConvNorm(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        )\n",
       "      )\n",
       "      (bilstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
       "      (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (duration_predictor_loss): AttributeRegressionLoss()\n",
       "  (speaker_embed_regularization_loss): VarianceCovarianceEmbeddingRegLoss()\n",
       "  (speaker_accent_cross_regularization_loss): AttributeMinCrossCovarianceRegLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d591450f-0c47-44ce-8261-87d074e1007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "hey\n",
      "0\n",
      "{'script': ['{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}'], 'spk_id': tensor([1], device='cuda:0'), 'decoder_spk_id': tensor([1], device='cuda:0'), 'duration_spk_id': tensor([1], device='cuda:0'), 'f0_spk_id': tensor([1], device='cuda:0'), 'energy_spk_id': tensor([1], device='cuda:0'), 'accent_id': tensor([0], device='cuda:0'), 'text_encoded': tensor([[  0,  51, 178, 252,   0, 131,  64, 304, 114, 302,  71, 304, 252,   0,\n",
      "          88, 302, 164, 304,  86, 302,  40,  85, 176, 111,  25,   0, 131, 302,\n",
      "          40, 111,  71, 304,  86,   0,  51, 178, 252,   0,  73, 302, 150, 304,\n",
      "          88, 214, 146,   0, 187, 302, 164, 304,  95,  25,   0,  51,  31,  31,\n",
      "         146, 131, 302,  40,  88,  51, 179, 113, 176,   0,  51,  71, 304,   0,\n",
      "         131, 302, 135, 111, 113, 176, 214,  88,   0,  51, 178, 252,   0,  85,\n",
      "         302, 178, 153, 176, 266,  88, 113, 214,  88,   0,  51, 178, 252,   0,\n",
      "          43, 252, 302,  41, 113, 176,  25,   0,  43, 214, 111, 178, 252, 214,\n",
      "          88,   0,  51,  40, 111,   0,  85, 302,  40,  88, 113,   0,  51, 178,\n",
      "         111,   0,  73, 302, 150, 304,  88, 214, 146, 111,   0, 260, 302, 164,\n",
      "          86, 178, 146,   0,  73, 303, 164, 304,  86,  27,   0]],\n",
      "       device='cuda:0'), 'idx': tensor([0], device='cuda:0'), 'speaker_f0_mean': tensor([4.9206], device='cuda:0', dtype=torch.float64), 'speaker_f0_std': tensor([0.2015], device='cuda:0', dtype=torch.float64), 'language': ['de_DE']}\n"
     ]
    }
   ],
   "source": [
    "for k in inp1.keys():\n",
    "    if type(inp1[k]) == torch.Tensor:\n",
    "        inp1[k] = inp1[k].to(device=\"cuda\")\n",
    "        print(\"hey\")\n",
    "        print(inp1[k].get_device())\n",
    "print(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5e0bb2c-8b6f-4620-865f-68097c9d0625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.} is NOT phonemized...\n",
      "de_DE\n",
      "de_DE|{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m}{ ˈa l ə s,} {v ˈa s}{ iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə}{ ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə}{ ʊ n t}{ ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s}{ ɛ ɾ}{ ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}\n",
      "de_DE|[51, 178, 252, 0, 131, 64, 304, 114, 302, 71, 304, 252, 0, 88, 302, 164, 304, 86, 302, 40, 85, 176, 111, 25, 0, 131, 302, 40, 111, 71, 304, 86, 0, 51, 178, 252, 0, 73, 302, 150, 304, 88, 214, 146, 0, 187, 302, 164, 304, 95, 25, 0, 51, 31, 31, 146, 131, 302, 40, 88, 51, 179, 113, 176, 0, 51, 71, 304, 0, 131, 302, 135, 111, 113, 176, 214, 88, 0, 51, 178, 252, 0, 85, 302, 178, 153, 176, 266, 88, 113, 214, 88, 0, 51, 178, 252, 0, 43, 252, 302, 41, 113, 176, 25, 0, 43, 214, 111, 178, 252, 214, 88, 0, 51, 40, 111, 0, 85, 302, 40, 88, 113, 0, 51, 178, 111, 0, 73, 302, 150, 304, 88, 214, 146, 111, 0, 260, 302, 164, 86, 178, 146, 0, 73, 303, 164, 304, 86, 27]\n",
      "hui-berndungerer-other\n",
      "hui-berndungerer-other\n",
      "{'f0_median': 136.10675048828125, 'f0_mean': 140.10650634765625, 'f0_std': 33.18109893798828, 'log_f0_median': 4.9134392738342285, 'log_f0_mean': 4.920597553253174, 'log_f0_std': 0.20154008269309998, 'energy_mean': 0.9912275075912476, 'energy_std': 0.044663988053798676, 'n_files': 100}\n"
     ]
    }
   ],
   "source": [
    "inp1 = next(iter(a_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c5fb8f8-fb08-4da4-9707-dc1afe82c930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'script': ['{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}'], 'spk_id': tensor([1], device='cuda:0'), 'decoder_spk_id': tensor([1], device='cuda:0'), 'duration_spk_id': tensor([1], device='cuda:0'), 'f0_spk_id': tensor([1], device='cuda:0'), 'energy_spk_id': tensor([1], device='cuda:0'), 'accent_id': tensor([0], device='cuda:0'), 'text_encoded': tensor([[  0,  51, 178, 252,   0, 131,  64, 304, 114, 302,  71, 304, 252,   0,\n",
      "          88, 302, 164, 304,  86, 302,  40,  85, 176, 111,  25,   0, 131, 302,\n",
      "          40, 111,  71, 304,  86,   0,  51, 178, 252,   0,  73, 302, 150, 304,\n",
      "          88, 214, 146,   0, 187, 302, 164, 304,  95,  25,   0,  51,  31,  31,\n",
      "         146, 131, 302,  40,  88,  51, 179, 113, 176,   0,  51,  71, 304,   0,\n",
      "         131, 302, 135, 111, 113, 176, 214,  88,   0,  51, 178, 252,   0,  85,\n",
      "         302, 178, 153, 176, 266,  88, 113, 214,  88,   0,  51, 178, 252,   0,\n",
      "          43, 252, 302,  41, 113, 176,  25,   0,  43, 214, 111, 178, 252, 214,\n",
      "          88,   0,  51,  40, 111,   0,  85, 302,  40,  88, 113,   0,  51, 178,\n",
      "         111,   0,  73, 302, 150, 304,  88, 214, 146, 111,   0, 260, 302, 164,\n",
      "          86, 178, 146,   0,  73, 303, 164, 304,  86,  27,   0]],\n",
      "       device='cuda:0'), 'idx': tensor([0], device='cuda:0'), 'speaker_f0_mean': tensor([4.9206], device='cuda:0', dtype=torch.float64), 'speaker_f0_std': tensor([0.2015], device='cuda:0', dtype=torch.float64), 'language': ['de_DE']}\n",
      "\n",
      "\n",
      "\n",
      "raw text:  ['{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "raw t:  {d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}\n",
      "lang:  de_DE\n",
      "\n",
      "\n",
      "\n",
      "{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m} { ˈa l ə s,} {v ˈa s} { iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə} { ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə} { ʊ n t} { ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s} { ɛ ɾ} { ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.} is NOT phonemized...\n",
      "de_DE\n",
      "de_DE|{d ɛ ɾ} {v eː ts ˈiː ɾ} {n ˈɑː m}{ ˈa l ə s,} {v ˈa s}{ iː m} {d ɛ ɾ} {k ˈøː n ɪ ç} {ɡ ˈɑː p,} {d ?? ç v ˈa n d ɜ t ə} {d iː} {v ˈy s t ə}{ ɪ n} {d ɛ ɾ} {l ˈɛ ŋ ə}{ ʊ n t}{ ɪ n} {d ɛ ɾ} {b ɾ ˈaɪ t ə,} {b ɪ s}{ ɛ ɾ}{ ɪ n} {d a s} {l ˈa n t} {d ɛ s} {k ˈøː n ɪ ç s} {ʃ ˈɑ m ɛ ç} {k ˌɑː m.}\n",
      "de_DE|[51, 178, 252, 0, 131, 64, 304, 114, 302, 71, 304, 252, 0, 88, 302, 164, 304, 86, 302, 40, 85, 176, 111, 25, 0, 131, 302, 40, 111, 71, 304, 86, 0, 51, 178, 252, 0, 73, 302, 150, 304, 88, 214, 146, 0, 187, 302, 164, 304, 95, 25, 0, 51, 31, 31, 146, 131, 302, 40, 88, 51, 179, 113, 176, 0, 51, 71, 304, 0, 131, 302, 135, 111, 113, 176, 214, 88, 0, 51, 178, 252, 0, 85, 302, 178, 153, 176, 266, 88, 113, 214, 88, 0, 51, 178, 252, 0, 43, 252, 302, 41, 113, 176, 25, 0, 43, 214, 111, 178, 252, 214, 88, 0, 51, 40, 111, 0, 85, 302, 40, 88, 113, 0, 51, 178, 111, 0, 73, 302, 150, 304, 88, 214, 146, 111, 0, 260, 302, 164, 86, 178, 146, 0, 73, 303, 164, 304, 86, 27]\n",
      "....f0 tranformation...\n"
     ]
    }
   ],
   "source": [
    "model2.forward(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95eeac-9593-4f69-aa37-eacd963e5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "resyn_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbc627b-d8e0-4592-acc3-e3fce2e1f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some runtime error\n",
    "with open(\"/akshit/scratch/RAD-MMM/tutorials/run1/lightning_logs/version_12/hparams.yaml\", \"r\") as f:\n",
    "    hparams = yaml.safe_load(f)\n",
    "    \n",
    "# model2 = pytorch_lightning.cli.instantiate_module(TTSModel,config=hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55254e0b-8f9d-4337-9350-4e2b5df34e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model3=torch.load(attribute_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdccbda-22b0-4a16-abf9-6e79ee91e442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734de79-1886-4518-a5b6-328fc13e5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = torch.load(decoder_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ddc29-72a4-4d9c-b594-98178e163719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3901ffc-601e-4170-b9b6-5c184c74c739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run=True, like cmd line; https://lightning.ai/docs/pytorch/stable/cli/lightning_cli_advanced_3.html, fails with SIGSEGV\n",
    "# inp = [\"predict\", \"--config=\"+gen_config_path,\\\n",
    "#          # \"--ckpt_path=\"+attribute_model_path,\\\n",
    "#          \"--model.predict_mode=tts\", \\\n",
    "#          \"--data.inference_transcript='model_inputs/resynthesis_prompts.json'\", \\\n",
    "#          \"--model.prediction_output_dir='/akshit/scratch/RAD-MMM/tutorials/out1'\", \\\n",
    "#          \"--trainer.devices=1\", \"--data.batch_size=1\", \\\n",
    "#          \"--model.vocoder_checkpoint_path=\"+voc_model_path, \\\n",
    "#          \"--model.vocoder_config_path=\"+voc_config_path, \\\n",
    "#          \"--data.phonemizer_cfg=\"+'{\"en_US\": \"assets/en_US_word_ipa_map.txt\",\"es_MX\": \"assets/es_MX_word_ipa_map.txt\",\"de_DE\": \"assets/de_DE_word_ipa_map.txt\",\"en_UK\": \"assets/en_UK_word_ipa_map.txt\",\"es_CO\": \"assets/es_CO_word_ipa_map.txt\",\"es_ES\": \"assets/es_ES_word_ipa_map.txt\",\"fr_FR\": \"assets/fr_FR_word_ipa_map.txt\",\"hi_HI\": \"assets/hi_HI_word_ipa_map.txt\",\"pt_BR\": \"assets/pt_BR_word_ipa_map.txt\",\"te_TE\": \"assets/te_TE_word_ipa_map.txt\"}', \\\n",
    "#          \"--model.encoders_path=\"+decoder_model_path, \\\n",
    "#          \"--model.decoder_path=\"+decoder_model_path, \\\n",
    "#          \"--model.output_directory='/akshit/scratch/RAD-MMM/tutorials/run1'\"]\n",
    "# lcli(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d335a-b5a1-49d0-b3bd-1525d0f85af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Lightning-AI/pytorch-lightning/pull/18105 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the config files\n",
    "\n",
    "# with open(gen_config_path, \"r\") as f:\n",
    "#     gen_config = yaml.safe_load(f)\n",
    "    \n",
    "#     # set config\n",
    "#     gen_config[\"ckpt_path\"] = attribute_model_path\n",
    "#     gen_config[\"model\"][\"predict_mode\"]=\"tts\"\n",
    "#     gen_config[\"model\"][\"prediction_output_dir\"]=\"/akshit/scratch/RAD-MMM/tutorials/out1\" #ToDo\n",
    "#     gen_config[\"model\"][\"vocoder_checkpoint_path\"]=voc_model_path\n",
    "#     gen_config[\"model\"][\"vocoder_config_path\"]=voc_model_path\n",
    "#     gen_config[\"model\"][\"decoder_path\"]=decoder_model_path\n",
    "#     gen_config[\"model\"][\"encoders_path\"]=decoder_model_path\n",
    "#     gen_config[\"model\"][\"output_directory\"]=\"/akshit/scratch/RAD-MMM/tutorials/run1\"\n",
    "#     gen_config[\"trainer\"][\"devices\"]=1\n",
    "    # gen_config[\"data\"][\"inference_transcript\"]=\"model_inputs/resynthesis_prompts.json\" #ToDo\n",
    "    # gen_config[\"data\"][\"batch_size\"]=1\n",
    "    # gen_config[\"data\"][\"phonemizer_cfg\"]=phonemizer_cfg\n",
    "    \n",
    "#     # set defaults\n",
    "#     gen_config[\"checkpoint_callback\"][\"filename\"]=\"latest-epoch_{epoch}-iter_{global_step:.0f}\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"monitor\"]=\"global_step\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"mode\"]= \"max\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"every_n_train_steps\"]= 3000\n",
    "#     gen_config[\"checkpoint_callback\"][\"dirpath\"]= \"/debug\"\n",
    "#     gen_config[\"checkpoint_callback\"][\"save_top_k\"] = -1\n",
    "#     gen_config[\"checkpoint_callback\"][\"auto_insert_metric_name\"]=False\n",
    "    \n",
    "#     # linking args\n",
    "#     gen_config[\"model\"][\"phonemizer_cfg\"]=gen_config[\"data\"][\"phonemizer_cfg\"]\n",
    "#     gen_config[\"model\"][\"add_bos_eos_to_text\"]=gen_config[\"data\"][\"add_bos_eos_to_text\"]\n",
    "#     gen_config[\"model\"][\"append_space_to_text\"]=gen_config[\"data\"][\"append_space_to_text\"]\n",
    "#     gen_config[\"model\"][\"prepend_space_to_text\"]=gen_config[\"data\"][\"prepend_space_to_text\"]\n",
    "#     gen_config[\"model\"][\"handle_phoneme_ambiguous\"]=gen_config[\"data\"][\"handle_phoneme_ambiguous\"]\n",
    "#     gen_config[\"model\"][\"handle_phoneme\"]=gen_config[\"data\"][\"handle_phoneme\"]\n",
    "#     gen_config[\"model\"][\"p_phoneme\"]=gen_config[\"data\"][\"p_phoneme\"]\n",
    "#     gen_config[\"model\"][\"phoneme_dict_path\"]=gen_config[\"data\"][\"phoneme_dict_path\"]\n",
    "#     gen_config[\"model\"][\"heteronyms_path\"]=gen_config[\"data\"][\"heteronyms_path\"]\n",
    "#     gen_config[\"model\"][\"cleaner_names\"]=gen_config[\"data\"][\"cleaner_names\"]\n",
    "#     gen_config[\"model\"][\"symbol_set\"]=gen_config[\"data\"][\"symbol_set\"]\n",
    "#     gen_config[\"model\"][\"sampling_rate\"]=gen_config[\"data\"][\"sampling_rate\"]\n",
    "#     gen_config[\"checkpoint_callback\"][\"dirpath\"]=gen_config[\"model\"][\"output_directory\"]\n",
    "#     gen_config[\"trainer\"][\"default_root_dir\"]=gen_config[\"model\"][\"output_directory\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d01e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fb967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cli.model\n",
    "data_module = cli.datamodule\n",
    "trainer = cli.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config_init.model\n",
    "data_module = config_init.data\n",
    "trainer = config_init.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929189ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we go through one input at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resynthesis input\n",
    "resyn_input = {\n",
    "      \"script\": \"{ʈ ˈɪ h ə ɾ ˌi} {b ˈã dʰ} {b ˈʌ n n ˌeː} {k eː} {d ɔː ɾ ˈaː n} {s ˈʌ n t oː} {n ˈeː} {ʋ ɪ ɾ ˈoː dʰ} {ɟ ə ɾ ˈuː ɾ} {k ˈɪ j aː} {l ˈeː k ɪ n} {pʰ ˈɪ ɾ} {tʰ ˈʌ m} {ɡ ˈʌ eː}\",\n",
    "      \"spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"decoder_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"duration_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"energy_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"f0_spk_id\": \"indic-iiit-hyd-female\",\n",
    "      \"language\": \"hi_HI\",\n",
    "      \"emotion\": \"other\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c2f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a gradio interface to try out multiple combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a725a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
